{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "genres.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PeYZFcZfqDm"
      },
      "source": [
        "#Installing libraries\n",
        "!pip install transformers==3.0.2\n",
        "!pip install nlp==0.4.0\n",
        "!pip install pyarrow==0.16.0\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPJrfdXsatFu"
      },
      "source": [
        "import torch\n",
        "import nlp\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EgLgEVkfra5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f0cd30-75e3-4a33-8b76-fd0853da928e"
      },
      "source": [
        "#Befere running evaluation we have to convert tensorflow checkpoint into pytorch model.\n",
        "#See here: https://github.com/huggingface/transformers/blob/master/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XiwYcFffszU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d42979-2e0f-4d66-c4cc-f0206620c60e"
      },
      "source": [
        "#Import the tokenizer and the config file from drive\n",
        "#The config file can be download from this link: https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json\n",
        "\n",
        "spm_path = '/content/drive/MyDrive/Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks/finetuned-model/Pytorch-Model/dl4se_vocab.model'\n",
        "config_file = '/content/drive/MyDrive/Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks/finetuned-model/Pytorch-Model/config.json'\n",
        "config = T5Config.from_json_file(config_file)\n",
        "tokenizer = T5Tokenizer.from_pretrained(spm_path)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqYdPg2Rfuhw"
      },
      "source": [
        "# Change the prefix when want to evaluate different tasks\n",
        "# (1) generate small patch\n",
        "# (2) generate medium patch\n",
        "# (3) generate abt assert\n",
        "# (4) generate raw assert\n",
        "\n",
        "# If you're evaluating abt/raw assert generative tasks, change example['method'].lower() for the input_text and  example['assertion'].lower() for the target_text\n",
        "\n",
        "def add_eos_to_examples(example):\n",
        "    example['input_text'] = 'generate small patch: %s </s>' % example['buggy'].lower()\n",
        "    example['target_text'] = '%s </s>' % example['fixed'].lower()\n",
        "    return example\n",
        "\n",
        "\n",
        "def convert_to_features(example_batch):\n",
        "    input_encodings = tokenizer.batch_encode_plus(example_batch['input_text'], pad_to_max_length=True, max_length=512)\n",
        "    target_encodings = tokenizer.batch_encode_plus(example_batch['target_text'], pad_to_max_length=True, max_length=512)\n",
        "\n",
        "    encodings = {\n",
        "        'input_ids': input_encodings['input_ids'], \n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'target_ids': target_encodings['input_ids'],\n",
        "        'target_attention_mask': target_encodings['attention_mask']\n",
        "    }\n",
        "\n",
        "    return encodings"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q360jPaLfxMx"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Here we have to change the script for loading the dataset\n",
        "# Pick the script according to the task and load it on this colab instance\n",
        "# Make sure to load the test set as well; otherwise, it doesn't work.\n",
        "\n",
        "valid_dataset = nlp.load_dataset('/content/bfp_dataset_script.py', split=nlp.Split.TEST)\n",
        "\n",
        "\n",
        "# map add_eos_to_examples function to the dataset example wise \n",
        "valid_dataset = valid_dataset.map(add_eos_to_examples, load_from_cache_file=False)\n",
        "\n",
        "# map convert_to_features batch wise\n",
        "valid_dataset = valid_dataset.map(convert_to_features, batched=True, load_from_cache_file=False)\n",
        "\n",
        "\n",
        "columns = ['input_ids', 'target_ids', 'attention_mask','target_attention_mask']\n",
        "valid_dataset.set_format(type='torch', columns=columns)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Vgfly4f0JB"
      },
      "source": [
        "#The BATCH_SIZE must be set according to the available VRAM.\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9incoTgAwZ7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "90a2cc79-8c2f-4563-a366-d8b418563d30"
      },
      "source": [
        "#Let's import the ground truth from the test dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('test.tsv',header=None,sep='\\t')\n",
        "\n",
        "references=[]\n",
        "\n",
        "for item in df[1]:\n",
        "  references.append(item.lower())\n",
        "\n",
        "references[1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'public type_1 method_1 ( ) { type_1 output = var_1 [ var_2 ] ; if ( ( var_2 ) >= 0 ) { var_2 = ( var_2 ) - 1 ; } else { } return output ; }'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPA2aw6egEgr"
      },
      "source": [
        "#Set CUDA device to leverage GPU computation\n",
        "CUDA = torch.device(\"cuda\")\n",
        "\n",
        "finetuned_model_path = '/content/drive/MyDrive/Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks/finetuned-model/Pytorch-Model/model.bin'\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "        finetuned_model_path,\n",
        "        config=config\n",
        "        ).to(CUDA)\n",
        "        \n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KQ94kR2gJFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9558703-ab7e-4492-e3c1-ca290b389235"
      },
      "source": [
        "# Change the max_length in model.generate according to specific tasks\n",
        "# For bfp_small and bfp_medium we set respectively 128 and 256.\n",
        "# For both abt assert and raw assert tasks, we used 512 as max length\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "predictions = []\n",
        "\n",
        "BEAM_SIZE = 1\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for batch in tqdm(dataloader):\n",
        "\n",
        "      outs = model.generate(\n",
        "                          input_ids=batch['input_ids'].to(CUDA),\n",
        "                          attention_mask=batch['attention_mask'].to(CUDA),\n",
        "                          num_beams=BEAM_SIZE, \n",
        "                          max_length=128,\n",
        "                          num_return_sequences=BEAM_SIZE, \n",
        "                          early_stopping=True\n",
        "                          )\n",
        "    \n",
        "\n",
        "    \n",
        "      outs = [tokenizer.decode(ids, skip_special_tokens=True)  for ids in outs]\n",
        "      predictions.extend(outs)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 365/365 [11:01<00:00,  1.81s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLKmlAJJO1M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86661d04-1991-4c52-dc5d-3022b5f61e35"
      },
      "source": [
        "pred_refined = []\n",
        "for pred in predictions:\n",
        "    if len(pred)>=2:\n",
        "      if pred[0]=='\"':\n",
        "          pred = pred[1:]\n",
        "      if pred[-1]=='\"':\n",
        "          pred = pred[:-1]\n",
        "    pred_refined.append(pred)\n",
        "    \n",
        "len(pred_refined),len(predictions)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5835, 5835)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhskxiWlgA2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1408fb-6d54-4e38-a336-c12be877aeb0"
      },
      "source": [
        "counter_pred = 0\n",
        "\n",
        "mispred_list = []\n",
        "\n",
        "sanity_check_list = []\n",
        "\n",
        "idx = 0\n",
        "\n",
        "len_prediction=(len(pred_refined))\n",
        "\n",
        "for i in range(0, len_prediction, BEAM_SIZE):\n",
        "\n",
        "    items_to_analyze = pred_refined[i:i+BEAM_SIZE]\n",
        "    target_item = ''.join(references[idx].split(' '))\n",
        "    \n",
        "\n",
        "    for pred in items_to_analyze:\n",
        "        pred_ref = ''.join(pred.split(' '))\n",
        "        if pred_ref == target_item:\n",
        "            counter_pred+=1\n",
        "            sanity_check_list.append(pred_ref)\n",
        "            break\n",
        "        else:\n",
        "          mispred_list.append(pred)\n",
        "         \n",
        "          \n",
        "        \n",
        "    idx += 1\n",
        "\n",
        "print('% of perfect predictions: ',(counter_pred/len(references))*100 )\n",
        "print(counter_pred)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of perfect predictions:  10.676949443016282\n",
            "623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDm8vAvCgOCl"
      },
      "source": [
        "#SAVING RESULTS\n",
        "\n",
        "idx=0\n",
        "\n",
        "with open('/content/drive/My Drive/conf/results_final/assert/abt/predictions_5/mispredictions_5.txt', 'w') as f:\n",
        "    for i in range( 0, len(mispred_list), BEAM_SIZE):\n",
        "        \n",
        "        items_to_analyze = mispred_list[i:i+BEAM_SIZE]\n",
        "\n",
        "        f.write('\\n************\\n')\n",
        "        f.write(\"tgt: %s\\n\" % references[idx])\n",
        "        for (index,mispred) in enumerate(items_to_analyze):\n",
        "          f.write('[%s]: %s\\n' % (str(index),mispred) )\n",
        "        f.write('\\n************\\n')\n",
        "\n",
        "        idx+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls8LiUpVgRqo"
      },
      "source": [
        "with open('/content/drive/My Drive/conf/results_final/assert/abt/predictions_5/predictions_5.txt', 'w') as f:\n",
        "    for item in pred_refined:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLOxaqzJdQtt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}